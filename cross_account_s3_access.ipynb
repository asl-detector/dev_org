{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30c7f20",
   "metadata": {},
   "source": [
    "# Cross-Account S3 Bucket Access from SageMaker\n",
    "\n",
    "This notebook demonstrates how to access S3 buckets in different AWS accounts from a SageMaker notebook instance or SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759451a",
   "metadata": {},
   "source": [
    "## Understanding Cross-Account Access\n",
    "\n",
    "SageMaker notebooks run with the permissions of an execution role. To access cross-account resources, you need to:\n",
    "\n",
    "1. Have an IAM role in your account that can assume roles in the target account\n",
    "2. Have an IAM role in the target account that allows your account's role to assume it\n",
    "3. Explicitly assume the target account role in your code\n",
    "\n",
    "Our infrastructure includes:\n",
    "- `sagemaker_execution_role`: The role attached to your SageMaker instance\n",
    "- `data_lake_access_role`: Role in dev_org that can assume a role in data_org\n",
    "- `extrn_data_access_role`: Role in dev_org that can assume a role in data_org for external data\n",
    "- Corresponding roles in data_org that can be assumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a7520",
   "metadata": {},
   "source": [
    "## Get Current SageMaker Role ARN\n",
    "\n",
    "First, let's identify which role we're currently using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current SageMaker execution role ARN\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Current SageMaker execution role: {role}\")\n",
    "\n",
    "# Get current AWS account ID\n",
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "print(f\"Current AWS account ID: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce04ac",
   "metadata": {},
   "source": [
    "## Method 1: Assume Role and Create New Session\n",
    "\n",
    "This is the most common approach - assume the cross-account role and create a new boto3 session with temporary credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the roles we can assume\n",
    "# Replace with actual role ARNs from your terraform outputs\n",
    "data_lake_access_role_arn = f\"arn:aws:iam::{account_id}:role/data-science-data-lake-access\"\n",
    "extrn_data_access_role_arn = f\"arn:aws:iam::{account_id}:role/data-science-extrn-data-access\"\n",
    "\n",
    "# Function to assume a role and get temporary credentials\n",
    "def assume_role(role_arn, session_name=\"AssumeRoleSession\"):\n",
    "    sts_client = boto3.client('sts')\n",
    "    try:\n",
    "        response = sts_client.assume_role(\n",
    "            RoleArn=role_arn,\n",
    "            RoleSessionName=session_name\n",
    "        )\n",
    "        return response['Credentials']\n",
    "    except ClientError as e:\n",
    "        print(f\"Error assuming role {role_arn}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01865b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume data lake access role and create a session\n",
    "def get_data_lake_session():\n",
    "    credentials = assume_role(data_lake_access_role_arn, \"DataLakeAccessSession\")\n",
    "    if credentials:\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=credentials['AccessKeyId'],\n",
    "            aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "            aws_session_token=credentials['SessionToken']\n",
    "        )\n",
    "        return session\n",
    "    return None\n",
    "\n",
    "# Create a session with data lake access permissions\n",
    "data_lake_session = get_data_lake_session()\n",
    "if data_lake_session:\n",
    "    print(\"Successfully created data lake access session\")\n",
    "    # Create an S3 client using the new session\n",
    "    s3_data_lake = data_lake_session.client('s3')\n",
    "else:\n",
    "    print(\"Failed to create data lake access session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd150f",
   "metadata": {},
   "source": [
    "## Access Data Lake Processed Bucket\n",
    "\n",
    "Now we can access the processed data lake bucket in the data_org account using our assumed role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: List objects in the processed data lake bucket\n",
    "data_lake_processed_bucket = \"asl-dataset-data-lake-processed-asl-dataset-00\"\n",
    "\n",
    "try:\n",
    "    if data_lake_session:\n",
    "        # Use the session with assumed role permissions\n",
    "        response = s3_data_lake.list_objects_v2(\n",
    "            Bucket=data_lake_processed_bucket,\n",
    "            MaxKeys=10\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in response:\n",
    "            print(f\"Objects in {data_lake_processed_bucket}:\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\"- {obj['Key']} ({obj['Size']} bytes)\")\n",
    "        else:\n",
    "            print(f\"No objects found in {data_lake_processed_bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing data lake bucket: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c811f",
   "metadata": {},
   "source": [
    "## Access External Training Data Bucket\n",
    "\n",
    "Similarly, we can access the external training data bucket by assuming the appropriate role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume external data access role\n",
    "def get_extrn_data_session():\n",
    "    credentials = assume_role(extrn_data_access_role_arn, \"ExtrnDataAccessSession\")\n",
    "    if credentials:\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=credentials['AccessKeyId'],\n",
    "            aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "            aws_session_token=credentials['SessionToken']\n",
    "        )\n",
    "        return session\n",
    "    return None\n",
    "\n",
    "# Create a session with external data access permissions\n",
    "extrn_data_session = get_extrn_data_session()\n",
    "if extrn_data_session:\n",
    "    print(\"Successfully created external data access session\")\n",
    "    # Create an S3 client using this session\n",
    "    s3_extrn_data = extrn_data_session.client('s3')\n",
    "else:\n",
    "    print(\"Failed to create external data access session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb57d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: List objects in the external training data bucket\n",
    "extrn_training_data_bucket = \"asl-dataset-extrn-raw-training-asl-dataset-00\"\n",
    "\n",
    "try:\n",
    "    if extrn_data_session:\n",
    "        response = s3_extrn_data.list_objects_v2(\n",
    "            Bucket=extrn_training_data_bucket,\n",
    "            MaxKeys=10\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in response:\n",
    "            print(f\"Objects in {extrn_training_data_bucket}:\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\"- {obj['Key']} ({obj['Size']} bytes)\")\n",
    "        else:\n",
    "            print(f\"No objects found in {extrn_training_data_bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing external training data bucket: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2346f11f",
   "metadata": {},
   "source": [
    "## Method 2: Using a Boto3 Resource with Assumed Role\n",
    "\n",
    "Another approach is to create a specific S3 resource with the assumed role credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 resource with data lake access role\n",
    "def get_s3_resource_with_role(role_arn):\n",
    "    credentials = assume_role(role_arn)\n",
    "    if credentials:\n",
    "        s3_resource = boto3.resource(\n",
    "            's3',\n",
    "            aws_access_key_id=credentials['AccessKeyId'],\n",
    "            aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "            aws_session_token=credentials['SessionToken']\n",
    "        )\n",
    "        return s3_resource\n",
    "    return None\n",
    "\n",
    "# Get S3 resource with data lake access\n",
    "s3_data_lake_resource = get_s3_resource_with_role(data_lake_access_role_arn)\n",
    "if s3_data_lake_resource:\n",
    "    print(\"Successfully created S3 resource with data lake access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Download a CSV file from the data lake and load it into a pandas DataFrame\n",
    "def read_csv_from_s3(bucket, key, s3_resource=None):\n",
    "    try:\n",
    "        if s3_resource is None:\n",
    "            s3_resource = boto3.resource('s3')  # Uses default credentials\n",
    "            \n",
    "        obj = s3_resource.Object(bucket, key)\n",
    "        # Download file to local temporary file\n",
    "        local_file = f\"/tmp/{os.path.basename(key)}\"\n",
    "        obj.download_file(local_file)\n",
    "        \n",
    "        # Read into pandas\n",
    "        df = pd.read_csv(local_file)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV from S3: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (replace with an actual CSV path)\n",
    "csv_path = \"sample-dir/sample-data.csv\"  # Replace with actual path\n",
    "# Uncomment when you have an actual file to read\n",
    "# df = read_csv_from_s3(data_lake_processed_bucket, csv_path, s3_data_lake_resource)\n",
    "# if df is not None:\n",
    "#     print(f\"Successfully loaded data with shape {df.shape}\")\n",
    "#     display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecb569",
   "metadata": {},
   "source": [
    "## Creating a Helper Function for Data Scientists\n",
    "\n",
    "To simplify the process, you can create helper functions that data scientists can use without needing to understand the details of cross-account access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_data_org(data_type=\"processed\", prefix=\"\", limit=100):\n",
    "    \"\"\"Helper function to access data in data_org buckets\n",
    "    \n",
    "    Args:\n",
    "        data_type (str): Type of data to access - 'processed', 'external', or 'captured'\n",
    "        prefix (str): S3 prefix/folder to list\n",
    "        limit (int): Maximum number of objects to list\n",
    "        \n",
    "    Returns:\n",
    "        list: List of S3 object information dictionaries\n",
    "    \"\"\"\n",
    "    # Define bucket and role based on data type\n",
    "    if data_type == \"processed\":\n",
    "        bucket = \"asl-dataset-data-lake-processed-asl-dataset-00\"\n",
    "        role_arn = data_lake_access_role_arn\n",
    "    elif data_type == \"external\":\n",
    "        bucket = \"asl-dataset-extrn-raw-training-asl-dataset-00\"\n",
    "        role_arn = extrn_data_access_role_arn\n",
    "    elif data_type == \"captured\":\n",
    "        bucket = \"asl-dataset-captured-raw-training-asl-dataset-00\"\n",
    "        role_arn = extrn_data_access_role_arn\n",
    "    else:\n",
    "        print(f\"Unknown data type: {data_type}\")\n",
    "        return None\n",
    "    \n",
    "    # Assume the appropriate role\n",
    "    credentials = assume_role(role_arn)\n",
    "    if not credentials:\n",
    "        print(f\"Failed to assume role for {data_type} data\")\n",
    "        return None\n",
    "    \n",
    "    # Create S3 client with temporary credentials\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "    \n",
    "    # List objects\n",
    "    try:\n",
    "        if prefix:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=limit)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, MaxKeys=limit)\n",
    "            \n",
    "        if 'Contents' in response:\n",
    "            return response['Contents']\n",
    "        else:\n",
    "            print(f\"No objects found in {bucket} with prefix '{prefix}'\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {data_type} data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1941d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the helper function\n",
    "processed_data = get_data_from_data_org(\"processed\")\n",
    "if processed_data:\n",
    "    print(f\"Found {len(processed_data)} objects in the processed data lake\")\n",
    "    if len(processed_data) > 0:\n",
    "        # Show first few items\n",
    "        for item in processed_data[:3]:\n",
    "            print(f\"- {item['Key']} ({item['Size']} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34f54a",
   "metadata": {},
   "source": [
    "## Creating a High-Level Data Access API\n",
    "\n",
    "For even more convenience, you can create a class that handles all the cross-account access details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOrgAccess:\n",
    "    \"\"\"Class to simplify access to data in data_org buckets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "        self.data_lake_access_role_arn = f\"arn:aws:iam::{self.account_id}:role/data-science-data-lake-access\"\n",
    "        self.extrn_data_access_role_arn = f\"arn:aws:iam::{self.account_id}:role/data-science-extrn-data-access\"\n",
    "        \n",
    "        # Define buckets\n",
    "        self.processed_bucket = \"asl-dataset-data-lake-processed-asl-dataset-00\"\n",
    "        self.external_bucket = \"asl-dataset-extrn-raw-training-asl-dataset-00\"\n",
    "        self.captured_bucket = \"asl-dataset-captured-raw-training-asl-dataset-00\"\n",
    "        \n",
    "        # Sessions (created on demand)\n",
    "        self._data_lake_session = None\n",
    "        self._extrn_data_session = None\n",
    "    \n",
    "    def _assume_role(self, role_arn, session_name=\"AssumeRoleSession\"):\n",
    "        \"\"\"Assume a role and get temporary credentials\"\"\"\n",
    "        sts_client = boto3.client('sts')\n",
    "        try:\n",
    "            response = sts_client.assume_role(\n",
    "                RoleArn=role_arn,\n",
    "                RoleSessionName=session_name\n",
    "            )\n",
    "            return response['Credentials']\n",
    "        except ClientError as e:\n",
    "            print(f\"Error assuming role {role_arn}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def data_lake_session(self):\n",
    "        \"\"\"Get or create a session with data lake access\"\"\"\n",
    "        if self._data_lake_session is None:\n",
    "            credentials = self._assume_role(self.data_lake_access_role_arn, \"DataLakeAccessSession\")\n",
    "            if credentials:\n",
    "                self._data_lake_session = boto3.Session(\n",
    "                    aws_access_key_id=credentials['AccessKeyId'],\n",
    "                    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                    aws_session_token=credentials['SessionToken']\n",
    "                )\n",
    "        return self._data_lake_session\n",
    "    \n",
    "    @property\n",
    "    def extrn_data_session(self):\n",
    "        \"\"\"Get or create a session with external data access\"\"\"\n",
    "        if self._extrn_data_session is None:\n",
    "            credentials = self._assume_role(self.extrn_data_access_role_arn, \"ExtrnDataAccessSession\")\n",
    "            if credentials:\n",
    "                self._extrn_data_session = boto3.Session(\n",
    "                    aws_access_key_id=credentials['AccessKeyId'],\n",
    "                    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "                    aws_session_token=credentials['SessionToken']\n",
    "                )\n",
    "        return self._extrn_data_session\n",
    "    \n",
    "    def list_processed_data(self, prefix=\"\", limit=100):\n",
    "        \"\"\"List objects in the processed data lake bucket\"\"\"\n",
    "        if self.data_lake_session is None:\n",
    "            return None\n",
    "        \n",
    "        s3 = self.data_lake_session.client('s3')\n",
    "        try:\n",
    "            if prefix:\n",
    "                response = s3.list_objects_v2(Bucket=self.processed_bucket, Prefix=prefix, MaxKeys=limit)\n",
    "            else:\n",
    "                response = s3.list_objects_v2(Bucket=self.processed_bucket, MaxKeys=limit)\n",
    "                \n",
    "            return response.get('Contents', [])\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing processed data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_external_data(self, prefix=\"\", limit=100):\n",
    "        \"\"\"List objects in the external training data bucket\"\"\"\n",
    "        if self.extrn_data_session is None:\n",
    "            return None\n",
    "        \n",
    "        s3 = self.extrn_data_session.client('s3')\n",
    "        try:\n",
    "            if prefix:\n",
    "                response = s3.list_objects_v2(Bucket=self.external_bucket, Prefix=prefix, MaxKeys=limit)\n",
    "            else:\n",
    "                response = s3.list_objects_v2(Bucket=self.external_bucket, MaxKeys=limit)\n",
    "                \n",
    "            return response.get('Contents', [])\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing external data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def read_csv(self, bucket_type, key):\n",
    "        \"\"\"Read a CSV file from one of the data_org buckets\n",
    "        \n",
    "        Args:\n",
    "            bucket_type (str): 'processed', 'external', or 'captured'\n",
    "            key (str): S3 object key (path to the CSV file)\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame or None\n",
    "        \"\"\"\n",
    "        if bucket_type == \"processed\":\n",
    "            bucket = self.processed_bucket\n",
    "            session = self.data_lake_session\n",
    "        elif bucket_type in [\"external\", \"captured\"]:\n",
    "            bucket = self.external_bucket if bucket_type == \"external\" else self.captured_bucket\n",
    "            session = self.extrn_data_session\n",
    "        else:\n",
    "            print(f\"Unknown bucket type: {bucket_type}\")\n",
    "            return None\n",
    "        \n",
    "        if session is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            s3 = session.client('s3')\n",
    "            local_file = f\"/tmp/{os.path.basename(key)}\"\n",
    "            s3.download_file(bucket, key, local_file)\n",
    "            return pd.read_csv(local_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0635bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the DataOrgAccess class\n",
    "data_access = DataOrgAccess()\n",
    "\n",
    "# List processed data\n",
    "processed_data = data_access.list_processed_data(limit=5)\n",
    "if processed_data:\n",
    "    print(f\"Found {len(processed_data)} objects in processed data\")\n",
    "    for item in processed_data[:3]:\n",
    "        print(f\"- {item['Key']} ({item['Size']} bytes)\")\n",
    "\n",
    "# List external data\n",
    "external_data = data_access.list_external_data(limit=5)\n",
    "if external_data:\n",
    "    print(f\"\\nFound {len(external_data)} objects in external data\")\n",
    "    for item in external_data[:3]:\n",
    "        print(f\"- {item['Key']} ({item['Size']} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42822399",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "SageMaker does not automatically assume cross-account roles. You need to:\n",
    "\n",
    "1. Explicitly assume roles in your notebook code using `boto3.client('sts').assume_role()`\n",
    "2. Create sessions or clients with the temporary credentials\n",
    "3. Use those sessions when accessing cross-account resources\n",
    "\n",
    "The helper functions and class we've created make this easier for data scientists to use without understanding all the IAM and cross-account access details."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
